{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "import numpy as np\n",
                "import xgboost as xgb\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Exception ignored in: <function tqdm.__del__ at 0x148580220>\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py\", line 1148, in __del__\n",
                        "    self.close()\n",
                        "  File \"/opt/homebrew/lib/python3.11/site-packages/tqdm/notebook.py\", line 279, in close\n",
                        "    self.disp(bar_style='danger', check_delay=False)\n",
                        "    ^^^^^^^^^\n",
                        "AttributeError: 'tqdm_notebook' np.object has no attribute 'disp'\n"
                    ]
                }
            ],
            "source": [
                "def ts_array_create(data_list, time_seq_len, pred_time, features, ffill_cols=[],two_hot_cols=[],merged_cols=[]):\n",
                "\n",
                "    X_all = []\n",
                "    Y_all_cls = []\n",
                "    Y_all_fst = []\n",
                "    files_record = []\n",
                "    \n",
                "    def vecot_to_num(v):\n",
                "        num = 0.0\n",
                "        for i, t in enumerate(v):\n",
                "            if t != 0:\n",
                "                num = i+t\n",
                "                break\n",
                "        return num\n",
                "\n",
                "    def replace_zero_with_one(value):\n",
                "        if value == 0:\n",
                "            return 0\n",
                "        else:\n",
                "            return 1\n",
                "\n",
                "\n",
                "    count = 0\n",
                "    for file in tqdm(data_list):\n",
                "\n",
                "        df = pd.read_csv(file)\n",
                "\n",
                "        # Hard to change to a feature, delete it now.\n",
                "        del df['Timestamp'], df['PCI'], df['EARFCN'], df['NR-PCI']\n",
                "\n",
                "        # Two hot column\n",
                "        for col in two_hot_cols:\n",
                "            df[col] = df[col].apply(replace_zero_with_one)\n",
                "            \n",
                "        df[ffill_cols] = df[ffill_cols].replace(0, pd.NA)\n",
                "        df[ffill_cols] = df[ffill_cols].ffill()\n",
                "        for col in ffill_cols:\n",
                "            if not pd.notna(df[col].iloc[0]):\n",
                "                df = df[df[col].notna()]\n",
                "        df.reset_index(drop=True, inplace=True)\n",
                "        \n",
                "        X = df[features]\n",
                "        # Merged columns\n",
                "        for cols in merged_cols:\n",
                "            new_column = X[cols[:-1]].max(axis=1)\n",
                "            col_num = X.columns.get_loc(cols[0])\n",
                "            X = X.drop(cols[:-1], axis=1)\n",
                "            X.insert(col_num, cols[-1], new_column)\n",
                "        \n",
                "        target = ['RLF_II', 'RLF_III']\n",
                "        Y = df[target].copy()\n",
                "        Y['RLF'] = Y.apply(lambda row: max(row['RLF_II'], row['RLF_III']), axis=1)\n",
                "        Y.drop(columns=target, inplace=True)\n",
                "\n",
                "        Xt_list = []\n",
                "        Yt_list = []\n",
                "\n",
                "        for i in range(time_seq_len):\n",
                "            X_t = X.shift(periods=-i)\n",
                "            X_t = X_t.to_numpy()\n",
                "            Xt_list.append(X_t)\n",
                "\n",
                "        Xt_list = np.stack(Xt_list, axis=0)\n",
                "        Xt_list = np.transpose(Xt_list, (1,0,2))\n",
                "        Xt_list = Xt_list[:-(time_seq_len + pred_time -1), :, :]\n",
                "\n",
                "        for i in range(time_seq_len, time_seq_len+pred_time):\n",
                "            Y_t = Y.shift(periods=-i)\n",
                "            Y_t = Y_t.to_numpy()\n",
                "            Yt_list.append(Y_t)\n",
                "\n",
                "        Yt_list = np.stack(Yt_list, axis=0)\n",
                "        Yt_list = np.transpose(Yt_list, (1,0,2))\n",
                "        Yt_list = Yt_list[:-(time_seq_len + pred_time -1), :, :]\n",
                "        Yt_list = np.squeeze(Yt_list)\n",
                "        if pred_time == 1: \n",
                "            Yt_cls = np.where(Yt_list != 0, 1, 0) \n",
                "            Yt_fst = Yt_list\n",
                "        else: \n",
                "            Yt_cls = np.where((Yt_list != 0).any(axis=1), 1, 0)\n",
                "            Yt_fst = np.apply_along_axis(vecot_to_num, axis=1, arr=Yt_list)\n",
                "\n",
                "        X_all.append(Xt_list)\n",
                "        Y_all_cls.append(Yt_cls)\n",
                "        Y_all_fst.append(Yt_fst)\n",
                "        files_record.append((file, (count, count +len(Yt_cls))))\n",
                "        count += len(Yt_cls)\n",
                "        \n",
                "    X_all = np.concatenate(X_all, axis=0)\n",
                "    Y_all_cls = np.concatenate(Y_all_cls, axis=0)\n",
                "    Y_all_fst = np.concatenate(Y_all_fst, axis=0)\n",
                "    \n",
                "    return X_all, Y_all_cls, Y_all_fst, files_record"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "# performance\n",
                "def performance(model, dtest, y_test):\n",
                "    \n",
                "    y_pred_proba = model.predict(dtest)\n",
                "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
                "\n",
                "    ACC = accuracy_score(y_test, y_pred)\n",
                "    AUC = roc_auc_score(y_test, y_pred_proba)\n",
                "    AUCPR = average_precision_score(y_test, y_pred_proba)\n",
                "    P = precision_score(y_test, y_pred)\n",
                "    R = recall_score(y_test, y_pred)\n",
                "    F1 = f1_score(y_test, y_pred)\n",
                "\n",
                "    print(f\"Accuracy: {ACC}; AUC: {AUC}; AUCPR: {AUCPR}; P: {P}; R: {R}; F1: {F1}\")\n",
                "    \n",
                "    return ACC, AUC, AUCPR, P, R, F1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Debug Function\n",
                "def count_rlf(data_list):\n",
                "    count = 0\n",
                "    for f in data_list:\n",
                "        df = pd.read_csv(f)\n",
                "        for i in range(len(df)):\n",
                "            if df['RLF_II'].iloc[i] or df['RLF_III'].iloc[i]:\n",
                "                count += 1\n",
                "    return count\n",
                "\n",
                "def find_original_input(ind, file_record, time_seq_len):\n",
                "    \n",
                "    for (file, ind_range) in file_record:\n",
                "        if ind_range[0]<=ind<ind_range[1]:\n",
                "            target_file = file    \n",
                "            tar_ind_range = ind_range\n",
                "            \n",
                "    df = pd.read_csv(target_file)\n",
                "\n",
                "    return df[ind-tar_ind_range[0]:ind-tar_ind_range[0]+time_seq_len], target_file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set seed\n",
                "seed = 55688"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ImportError",
                    "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "\u001b[1;32m/Users/loret/Downloads/RLF_predict.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m two_hot_vec_cols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mE-UTRAN-eventA3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39meventA5\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mNR-eventA3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39meventB1-NR-r15\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mLTE_HO\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mMN_HO\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mMN_HO_to_eNB\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mSN_setup\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mSN_Rel\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mSN_HO\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRLF_II\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mRLF_III\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mSCG_RLF\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m merged_cols \u001b[39m=\u001b[39m [[\u001b[39m'\u001b[39m\u001b[39mLTE_HO\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMN_HO_to_eNB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLTE_HO\u001b[39m\u001b[39m'\u001b[39m], [\u001b[39m'\u001b[39m\u001b[39mRLF_II\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRLF_III\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRLF\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m X_train, y_cls_train, y_fst_train, record_train \u001b[39m=\u001b[39m ts_array_create(train_data_list, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m X_train_2d \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mreshape(X_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m X_test1, y_cls_test1, y_fst_test1, record_test1 \u001b[39m=\u001b[39m ts_array_create(test_data_list1, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
                        "\u001b[1;32m/Users/loret/Downloads/RLF_predict.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m tqdm(data_list):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/loret/Downloads/RLF_predict.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# Hard to change to a feature, delete it now.\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m unit_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munit_scale \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m    233\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39m*\u001b[39m unit_scale \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n\u001b[0;32m--> 234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstatus_printer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp, total, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdesc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mncols)\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mpbar \u001b[39m=\u001b[39m proxy(\u001b[39mself\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplayed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m IProgress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m total:\n\u001b[1;32m    110\u001b[0m     pbar \u001b[39m=\u001b[39m IProgress(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mmax\u001b[39m\u001b[39m=\u001b[39mtotal)\n",
                        "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
                    ]
                }
            ],
            "source": [
                "# Read Data\n",
                "data_folder = '/Users/loret/Downloads/v22'\n",
                "data_list = [os.path.join(data_folder, file) for file in os.listdir(data_folder)]\n",
                "data_list.remove(os.path.join(data_folder, 'record.csv'))\n",
                "test_data_list1 = [x for x in data_list if ('2023-11-01' in x and '#02' in x)] # 同一天機捷 \n",
                "test_data_list2 = [x for x in data_list if '2023-11-02' in x] # 機捷\n",
                "test_data_list3 = [x for x in data_list if '2023-11-09' in x] # 棕線\n",
                "test_data_list4 = test_data_list1 + test_data_list2 + test_data_list3\n",
                "train_data_list = [x for x in data_list if x not in test_data_list1 + test_data_list2 + test_data_list3]\n",
                "\n",
                "time_seq_len = 10\n",
                "pred_time = 3\n",
                "\n",
                "features = ['num_of_neis', 'RSRP','RSRQ','RSRP1','RSRQ1','nr-RSRP','nr-RSRQ','nr-RSRP1','nr-RSRQ1',\n",
                "            'E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
                "            'LTE_HO','MN_HO','MN_HO_to_eNB','SN_setup','SN_Rel','SN_HO', \n",
                "            'RLF_II', 'RLF_III','SCG_RLF']\n",
                "ffill_cols = ['RSRP1', 'RSRQ1']\n",
                "two_hot_vec_cols = ['E-UTRAN-eventA3','eventA5','NR-eventA3','eventB1-NR-r15',\n",
                "            'LTE_HO','MN_HO','MN_HO_to_eNB','SN_setup','SN_Rel','SN_HO','RLF_II','RLF_III','SCG_RLF']\n",
                "merged_cols = [['LTE_HO', 'MN_HO_to_eNB', 'LTE_HO'], ['RLF_II', 'RLF_III', 'RLF']]\n",
                "\n",
                "X_train, y_cls_train, y_fst_train, record_train = ts_array_create(train_data_list, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
                "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
                "\n",
                "X_test1, y_cls_test1, y_fst_test1, record_test1 = ts_array_create(test_data_list1, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
                "X_test1_2d = X_test1.reshape(X_test1.shape[0], -1)\n",
                "\n",
                "X_test2, y_cls_test2, y_fst_test2, record_test2 = ts_array_create(test_data_list2, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
                "X_test2_2d = X_test2.reshape(X_test2.shape[0], -1)\n",
                "\n",
                "X_test3, y_cls_test3, y_fst_test3, record_test3 = ts_array_create(test_data_list3, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
                "X_test3_2d = X_test3.reshape(X_test3.shape[0], -1)\n",
                "\n",
                "X_test4, y_cls_test4, y_fst_test4, record_test4 = ts_array_create(test_data_list4, time_seq_len, pred_time, features, ffill_cols,two_hot_vec_cols,merged_cols)\n",
                "X_test4_2d = X_test4.reshape(X_test4.shape[0], -1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train[0].shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count RLF number\n",
                "rlf_num_train = count_rlf(train_data_list)\n",
                "rlf_num_test1 = count_rlf(test_data_list1)\n",
                "rlf_num_test2 = count_rlf(test_data_list2)\n",
                "rlf_num_test3 = count_rlf(test_data_list3)\n",
                "rlf_num_test4 = count_rlf(test_data_list4)\n",
                "print(f'RLF # in training data: {rlf_num_train}\\nRLF # in testing data1: {rlf_num_test1}\\nRLF # in testing data2: {rlf_num_test2}\\nRLF # in testing data3: {rlf_num_test3}\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 將數據轉換為 DMatrix 格式\n",
                "dtrain = xgb.DMatrix(X_train_2d, label=y_cls_train)\n",
                "dtest1 = xgb.DMatrix(X_test1_2d, label=y_cls_test1)\n",
                "dtest2 = xgb.DMatrix(X_test2_2d, label=y_cls_test2)\n",
                "dtest3 = xgb.DMatrix(X_test3_2d, label=y_cls_test3)\n",
                "dtest4 = xgb.DMatrix(X_test4_2d, label=y_cls_test4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# xgb parameters\n",
                "params = {\n",
                "    'objective': 'binary:logistic', \n",
                "    'eval_metric': 'error',  \n",
                "    'max_depth': 8,\n",
                "    'learning_rate': 0.05,\n",
                "    'subsample': 1,\n",
                "    'colsample_bytree': 0.8,\n",
                "    'alpha': 0.01,\n",
                "    'lambda':1.0,\n",
                "    'seed': seed,\n",
                "    'tree_method': 'hist',\n",
                "    'device': 'cuda:0'\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model Create\n",
                "num_rounds = 1000\n",
                "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
                "model = xgb.train(params, dtrain, num_rounds, evals=watchlist, early_stopping_rounds=20,  verbose_eval=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Metric calculate\n",
                "performance(model, dtrain, y_cls_train)\n",
                "performance(model, dtest1, y_cls_test1)\n",
                "performance(model, dtest2, y_cls_test2)\n",
                "performance(model, dtest3, y_cls_test3)\n",
                "performance(model, dtest4, y_cls_test4)\n",
                "pass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save model\n",
                "save_path = '../model/rlf_cls_xgb.json'\n",
                "config = model.save_model(save_path)\n",
                "\n",
                "# how to load\n",
                "# model2 = xgb.Booster()\n",
                "# model2.load_model(save_path)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# xgb parameters\n",
                "params = {\n",
                "    'objective': 'binary:logistic', \n",
                "    'eval_metric': 'error',  \n",
                "    'max_depth': 8,\n",
                "    'learning_rate': 0.1,\n",
                "    'subsample': 0.8, # on data\n",
                "    'colsample_bytree': 0.8, # on feature\n",
                "    'lambda': 0.01, # L2\n",
                "    'alpha': 0.01, # L1\n",
                "    'seed': seed,\n",
                "    'tree_method': 'hist',\n",
                "    'device': 'cuda:0'\n",
                "}\n",
                "\n",
                "# Model Create\n",
                "num_rounds = 200\n",
                "# watchlist = [(dtrain, 'train')]\n",
                "# watchlist = [(dtrain, 'train'), (dtest4, 'test4')] \n",
                "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
                "model = xgb.train(params, dtrain, num_rounds, evals=watchlist,  early_stopping_rounds=20, verbose_eval=True)\n",
                "\n",
                "# Metric calculate\n",
                "performance(model, dtrain, y_cls_train)\n",
                "performance(model, dtest1, y_cls_test1)\n",
                "performance(model, dtest2, y_cls_test2)\n",
                "performance(model, dtest3, y_cls_test3)\n",
                "performance(model, dtest4, y_cls_test4)\n",
                "pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Grid Search"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from itertools import product\n",
                "\n",
                "watchlist = [(dtrain, 'train'), (dtest1, 'test1'), (dtest2, 'test2'), (dtest3, 'test3')] \n",
                "    \n",
                "# Define parameters range\n",
                "learning_rate_values = [0.05, 0.1, 0.2]\n",
                "max_depth_values = [4, 5, 6, 7, 8]\n",
                "subsample_values = [0.8, 0.9, 1.0]\n",
                "colsample_bytree_values = [0.8, 0.9, 1.0]\n",
                "alphas = [0.01,0.1,1]\n",
                "lambdas = [0.01,0.1,1]\n",
                "num_rounds_values = [50,100,200,300]\n",
                "r = product(learning_rate_values, max_depth_values, subsample_values, colsample_bytree_values, alphas, lambdas)\n",
                "\n",
                "savefile = '../info/xgb_record_no_early_stopping.csv'\n",
                "with open(savefile, 'w') as f:\n",
                "    print('lr, max_d, s_sample, cols_bytree, alpha, lambda, n,ACC(train), AUC(train), AUCPR(train), P(train), R(train), F1(train), ACC(train), AUC(test), AUCPR(test), P(test), R(test), F1(test)',file=f)\n",
                "    for lr, d, s, cbt, a, l in tqdm(r):\n",
                "        params = {'objective': 'binary:logistic', 'eval_metric': 'error',  'seed': seed,'tree_method': 'hist','device': 'cuda:0'}\n",
                "        params['learning_rate'] = lr\n",
                "        params['max_depth'] = d\n",
                "        params['subsample'] = s\n",
                "        params['colsample_bytree'] = cbt\n",
                "        params['alpha'] = a\n",
                "        params['lambda'] = l\n",
                "        for num_rounds in num_rounds_values:\n",
                "            model = xgb.train(params, dtrain, num_rounds, evals=watchlist,  early_stopping_rounds=20, verbose_eval=False)\n",
                "            \n",
                "            record = [lr, d, s, cbt, a, l, num_rounds]\n",
                "            record+=list(performance(model, dtrain, y_cls_train))\n",
                "            performance(model, dtest1, y_cls_test1)\n",
                "            performance(model, dtest2, y_cls_test2)\n",
                "            performance(model, dtest3, y_cls_test3)\n",
                "            record += list(performance(model, dtest4, y_cls_test4))\n",
                "            \n",
                "            params.clear()\n",
                "            record = [str(x) for x in record]\n",
                "            \n",
                "            print(','.join(record),end='\\n', file=f)\n",
                "            "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params = {\n",
                "    'objective': 'binary:logistic', \n",
                "    'eval_metric': 'error',  \n",
                "    'max_depth': 8,\n",
                "    'learning_rate': 0.1,\n",
                "    'subsample': 0.8, # on data\n",
                "    'colsample_bytree': 0.8, # on feature\n",
                "    'lambda': 0, # L2\n",
                "    'alpha': 0, # L1\n",
                "    'seed': seed,\n",
                "    'tree_method': 'hist',\n",
                "    'device': 'cuda:0'\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "params.clear()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Debugging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Spefify \n",
                "dtest = dtest4\n",
                "y_cls, y_fst, X_test = y_cls_test4, y_fst_test4, X_test4\n",
                "\n",
                "y_pred_proba = model.predict(dtest) \n",
                "y_pred = (y_pred_proba > 0.5).astype(int)\n",
                "\n",
                "FP_input = []\n",
                "FP_input_ind = []\n",
                "FP_time = []\n",
                "\n",
                "FN_input = []\n",
                "FN_input_ind = []\n",
                "\n",
                "TP_input = []\n",
                "TP_input_ind = []\n",
                "TP_time = []\n",
                "\n",
                "for i, (pred, label, t, x) in enumerate(zip(y_pred, y_cls, y_fst, X_test)):\n",
                "    if pred != label:\n",
                "        if label == 1: # FP analysis\n",
                "            FP_time.append(t)\n",
                "            x = pd.DataFrame(x, columns=features)\n",
                "            FP_input.append(x)\n",
                "            FP_input_ind.append(i)\n",
                "        else: # FN analysis\n",
                "            x = pd.DataFrame(x, columns=features)\n",
                "            FN_input.append(x)\n",
                "            FN_input_ind.append(i)\n",
                "    else: \n",
                "        if label == 1: # TP analysis\n",
                "            TP_time.append(t)\n",
                "            x = pd.DataFrame(x, columns=features)\n",
                "            TP_input.append(x)\n",
                "            TP_input_ind.append(i)\n",
                "            \n",
                "len(TP_input), len(FP_input), len(FN_input)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Excel filename\n",
                "file_record = record_test4\n",
                "excel_file = '../info/TP_data.xlsx'\n",
                "input_ind = TP_input_ind\n",
                "\n",
                "# ExcelWriter \n",
                "with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
                "    for ind in tqdm(input_ind):\n",
                "        # x = X_test[ind] \n",
                "        # df = pd.DataFrame(x, columns=features)\n",
                "        df, tar_file = find_original_input(ind, file_record, time_seq_len)\n",
                "        df['filename'] = [tar_file] + [None]*(len(df) - 1)\n",
                "        df.to_excel(writer, sheet_name=f'{ind}', index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df, _ = find_original_input(316, file_record, time_seq_len)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pprint import pprint\n",
                "pprint(TP_input_ind[:20])\n",
                "pprint(TP_time[:20])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = X_test4_2d[621]\n",
                "x = np.expand_dims(x, axis=0)\n",
                "X = xgb.DMatrix(x)\n",
                "model.predict(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(FN_input)/1490"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Failed CDF\n",
                "sorted_data = np.sort(FP_time)\n",
                "cumulative_distribution = np.arange(1, len(sorted_data) + 1) / (len(sorted_data)+len(FN_input))\n",
                "\n",
                "plt.ylim([0,1])\n",
                "plt.plot(sorted_data, cumulative_distribution, marker='o', linestyle='-', color='b')\n",
                "plt.xlabel('Time away from RLF (second)')\n",
                "# plt.ylabel('Cumulative Distribution Function (CDF)')\n",
                "plt.title('CDF of the false prediced Data')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# feature importance\n",
                "importance = model.get_score(importance_type='gain')\n",
                "\n",
                "sorted_importance = {}\n",
                "for k, v in importance.items():\n",
                "    num = int(k[1:])\n",
                "    feature_name = features[num%len(features)]\n",
                "    sorted_importance[f'{feature_name} {time_seq_len-num//34}'] = v\n",
                "\n",
                "sorted_importance = sorted(sorted_importance.items(), key=lambda x: x[1], reverse=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Feature Importance\n",
                "top_features = 20\n",
                "\n",
                "data, labels = [], []\n",
                "for f, score in reversed(sorted_importance[:top_features]):\n",
                "    data.append(round(score))\n",
                "    labels.append(f)\n",
                "\n",
                "bars = plt.barh(labels, data)\n",
                "plt.bar_label(bars)\n",
                "\n",
                "# 設置標題和標籤\n",
                "plt.title('Feature importance')\n",
                "plt.xlabel('F score (gain)')\n",
                "plt.ylabel('Features')\n",
                "\n",
                "plt.grid()\n",
                "\n",
                "# 顯示圖表\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pytorch",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}